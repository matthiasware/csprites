{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee50489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Beta\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from utils import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = torch.Tensor([.1]), torch.Tensor([.9])\n",
    "d = Beta(a, b)\n",
    "n_samples = 100\n",
    "x = d.sample((n_samples, ))\n",
    "simplex_plot(x)\n",
    "plot_beta_pdf(d, \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, b1 = torch.Tensor([.1]), torch.Tensor([.9])\n",
    "a2, b2 = 1/a1, 1/b1\n",
    "#\n",
    "d1 = Beta(a1, b1)\n",
    "d2 = Beta(a2, b2)\n",
    "#\n",
    "n_samples = 100\n",
    "x1 = d1.sample((n_samples, ))\n",
    "x2 = d2.sample((n_samples, ))\n",
    "#\n",
    "simplex_plot(x1)\n",
    "simplex_plot(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_beta_beta_pt(d2, d1))\n",
    "print(kl_beta_beta_pt(d1, d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_beta_beta(ab_aprx=(a2, b2), ab_true=(a1, b1), forward=True))\n",
    "print(kl_beta_beta(ab_aprx=(a2, b2), ab_true=(a1, b1), forward=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "#\n",
    "a_true, b_true = 0.2, 0.5\n",
    "d_true = Beta(a_true, b_true)\n",
    "#\n",
    "d_aprx = Beta(1/b_true, 1/a_true)\n",
    "x_aprx = d_aprx.sample((n_samples,))\n",
    "#\n",
    "a_aprx, b_aprx = beta_params(x_aprx)\n",
    "a_aprx, b_aprx = 1/b_aprx, 1/a_aprx\n",
    "#\n",
    "print(\"P = Beta({:.3f},{:.3f})\".format(a_true, b_true))\n",
    "print(\"Q = Beta({:.3f},{:.3f})\".format(a_aprx, b_aprx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_pdf(d_true, title=\"True\")\n",
    "plot_beta_pdf(d_aprx, title=\"Approx\")\n",
    "plot_beta_pdf(d_aprx, title=\"Estimated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4292b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, d_in, d_out, batch_norm):\n",
    "        super().__init__()\n",
    "        if batch_norm:\n",
    "            self.ff = nn.Sequential(\n",
    "                nn.Linear(d_in, d_out),\n",
    "                nn.BatchNorm1d(d_out),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.ff = nn.Sequential(\n",
    "                nn.Linear(d_in, d_out),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, n_hid, d_out, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.n_hid = n_hid\n",
    "        self.d_hid = d_hid\n",
    "        \n",
    "        self.dims = [d_in] + [d_hid] * n_hid\n",
    "        \n",
    "        layers = []\n",
    "        for idx in range(len(self.dims) - 1):\n",
    "            layers.append(BasicBlock(self.dims[idx], self.dims[idx + 1], batch_norm))\n",
    "        layers.append(nn.Linear(self.dims[-1], d_out))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73aee9e",
   "metadata": {},
   "source": [
    "# MIN DKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 1\n",
    "d_hid = 16\n",
    "d_out = 1\n",
    "n_hid = 4\n",
    "batch_norm = True\n",
    "#\n",
    "a_true, b_true = torch.Tensor([0.1, 0.5])\n",
    "#\n",
    "batch_size = 512\n",
    "num_steps = 400\n",
    "num_epochs = 10\n",
    "plot_freq = 5\n",
    "#\n",
    "dist_true = Beta(a_true, b_true)\n",
    "plot_beta_pdf(dist_true, \"True\")\n",
    "#\n",
    "dist_in = torch.distributions.Uniform(0, 1)\n",
    "x = dist_in.sample((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "model.apply(init_weights)\n",
    "x = torch.rand((512, d_in))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x)\n",
    "#\n",
    "simplex_plot(z.detach().numpy())\n",
    "simplex_plot(x.numpy())\n",
    "print(z.min(), z.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884748e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "all_loss = []\n",
    "all_a = []\n",
    "all_b = []\n",
    "for epoch_idx in range(1, num_epochs + 1, 1):\n",
    "    model.train()\n",
    "    desc = \"Epoch [{:3}/{:3}] {}:\".format(epoch_idx, num_epochs, 'train')\n",
    "    pbar = tqdm(range(num_steps), bar_format= desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    epoch_loss = 0\n",
    "    epoch_a = 0\n",
    "    epoch_b = 0\n",
    "    for step in pbar:\n",
    "        x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "        # ##########\n",
    "        # TRAIN\n",
    "        # ##########\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        z = model(x)\n",
    "        a_z, b_z = beta_params(z)\n",
    "        loss =  0.5 * (kl_beta_beta((a_z,b_z),(a_true,b_true),forward=True) + kl_beta_beta((a_z,b_z),(a_true,b_true),forward=False))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_a += a_z.item()\n",
    "        epoch_b += b_z.item()\n",
    "        pbar.set_postfix({'loss': loss.item(), 'a': a_z.item(), 'b': b_z.item()})\n",
    "    all_loss.append(epoch_loss / num_steps)\n",
    "    all_a.append(epoch_a / num_steps)\n",
    "    all_b.append(epoch_b / num_steps)\n",
    "    ############\n",
    "    # EVAL\n",
    "    ############\n",
    "    if epoch_idx % plot_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "            z = model(x)\n",
    "            #\n",
    "            a_z, b_z = beta_params(z)\n",
    "            dist_aprx = Beta(a_z, b_z)\n",
    "            plot_beta_pdf(dist_aprx, \"aprx - a={} b={}\".format(a_z.item(), b_z.item()))\n",
    "            simplex_plot(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dbb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b85b304",
   "metadata": {},
   "source": [
    "# Beta: DKL(P_hat, Q_hat) vs DKL(P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea984e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 1\n",
    "d_hid = 16\n",
    "d_out = 1\n",
    "n_hid = 4\n",
    "batch_norm = True\n",
    "#\n",
    "a_true, b_true = torch.Tensor([0.1, 0.9])\n",
    "#\n",
    "batch_size = 64\n",
    "num_steps = 400\n",
    "num_epochs = 10\n",
    "plot_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40070df2",
   "metadata": {},
   "source": [
    "# DKL(Q_hat,P_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true_prox = torch.Tensor([1 / b_true])\n",
    "b_true_prox = torch.Tensor([1 / a_true])\n",
    "#\n",
    "dist_true = Beta(a_true, b_true)\n",
    "dist_true_prox = Beta(a_true_prox, b_true_prox)\n",
    "#\n",
    "plot_beta_pdf(dist_true, \"True\")\n",
    "plot_beta_pdf(dist_true_prox, \"Prox\")\n",
    "#\n",
    "dist_in = torch.distributions.Uniform(0, 1)\n",
    "x = dist_in.sample((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4be247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "model.apply(init_weights)\n",
    "x = torch.rand((512, d_in))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x)\n",
    "#\n",
    "simplex_plot(z.detach().numpy())\n",
    "simplex_plot(x.numpy())\n",
    "print(z.min(), z.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e78bc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "all_loss = []\n",
    "all_a = []\n",
    "all_b = []\n",
    "for epoch_idx in range(1, num_epochs + 1, 1):\n",
    "    model.train()\n",
    "    desc = \"Epoch [{:3}/{:3}] {}:\".format(epoch_idx, num_epochs, 'train')\n",
    "    pbar = tqdm(range(num_steps), bar_format= desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    epoch_loss = 0\n",
    "    epoch_a = 0\n",
    "    epoch_b = 0\n",
    "    for step in pbar:\n",
    "        x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "        # ##########\n",
    "        # TRAIN\n",
    "        # ##########\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        z = model(x)\n",
    "        a_z, b_z = beta_params(z)\n",
    "        a_aprx_prox = 1 / b_z\n",
    "        b_aprx_prox = 1 / a_z\n",
    "        loss =  kl_beta_beta((a_aprx_prox,b_aprx_prox),\n",
    "                                 (a_true_prox,b_true_prox),forward=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_a += a_z.item()\n",
    "        epoch_b += b_z.item()\n",
    "        pbar.set_postfix({'loss': loss.item(), 'a': a_z.item(), 'b': b_z.item()})\n",
    "        #time.sleep(0.001)\n",
    "    all_loss.append(epoch_loss / num_steps)\n",
    "    all_a.append(epoch_a / num_steps)\n",
    "    all_b.append(epoch_b / num_steps)\n",
    "    ############\n",
    "    # EVAL\n",
    "    ############\n",
    "    if epoch_idx % plot_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "            z = model(x)\n",
    "            #\n",
    "            a_z, b_z = beta_params(z)\n",
    "            dist_aprx = Beta(a_z, b_z)\n",
    "            plot_beta_pdf(dist_aprx, \"aprx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ffe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(num_epochs)\n",
    "plt.plot(xx, all_loss)\n",
    "plt.show()\n",
    "plt.plot(xx, np.abs(np.array(all_a) - a_true.numpy()))\n",
    "plt.plot(xx, np.abs(np.array(all_b) - b_true.numpy()))\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "n_samples = 64\n",
    "with torch.no_grad():\n",
    "    x = dist_in.sample((n_samples, )).view((-1, 1))\n",
    "    z = model(x)\n",
    "simplex_plot(z.numpy())\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "with torch.no_grad():\n",
    "    x = dist_in.sample((n_samples, )).view((-1, 1))\n",
    "    z = model(x)\n",
    "\n",
    "#\n",
    "a_z, b_z = beta_params(z)\n",
    "#\n",
    "dist_aprx = Beta(a_z, b_z)\n",
    "plot_beta_pdf(dist_aprx, \"aprx\")\n",
    "#\n",
    "print(\"pred\", a_z, b_z)\n",
    "print(\"true\",a_true, b_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee44479",
   "metadata": {},
   "source": [
    "# DKL(Q,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_true = Beta(a_true, b_true)\n",
    "plot_beta_pdf(dist_true, \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "model.apply(init_weights)\n",
    "x = torch.rand((512, d_in))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x)\n",
    "#\n",
    "simplex_plot(z.detach().numpy())\n",
    "simplex_plot(x.numpy())\n",
    "print(z.min(), z.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1e87e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "all_loss = []\n",
    "all_a = []\n",
    "all_b = []\n",
    "for epoch_idx in range(1, num_epochs + 1, 1):\n",
    "    model.train()\n",
    "    desc = \"Epoch [{:3}/{:3}] {}:\".format(epoch_idx, num_epochs, 'train')\n",
    "    pbar = tqdm(range(num_steps), bar_format= desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    epoch_loss = 0\n",
    "    epoch_a = 0\n",
    "    epoch_b = 0\n",
    "    for step in pbar:\n",
    "        x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "        # ##########\n",
    "        # TRAIN\n",
    "        # ##########\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        z = model(x)\n",
    "        a_z, b_z = beta_params(z)\n",
    "        loss =  kl_beta_beta((a_z,b_z),\n",
    "                             (a_true,b_true),forward=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_a += a_z.item()\n",
    "        epoch_b += b_z.item()\n",
    "        pbar.set_postfix({'loss': loss.item(), 'a': a_z.item(), 'b': b_z.item()})\n",
    "        #time.sleep(0.001)\n",
    "    all_loss.append(epoch_loss / num_steps)\n",
    "    all_a.append(epoch_a / num_steps)\n",
    "    all_b.append(epoch_b / num_steps)\n",
    "    ############\n",
    "    # EVAL\n",
    "    ############\n",
    "    if epoch_idx % plot_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "            z = model(x)\n",
    "            #\n",
    "            a_z, b_z = beta_params(z)\n",
    "            dist_aprx = Beta(a_z, b_z)\n",
    "            plot_beta_pdf(dist_aprx, \"aprx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6940ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(num_epochs)\n",
    "plt.plot(xx, all_loss)\n",
    "plt.show()\n",
    "plt.plot(xx, np.abs(np.array(all_a) - a_true.numpy()))\n",
    "plt.plot(xx, np.abs(np.array(all_b) - b_true.numpy()))\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "n_samples = 64\n",
    "with torch.no_grad():\n",
    "    x = dist_in.sample((n_samples, )).view((-1, 1))\n",
    "    z = model(x)\n",
    "simplex_plot(z.numpy())\n",
    "\n",
    "\n",
    "n_samples = 10000\n",
    "with torch.no_grad():\n",
    "    x = dist_in.sample((n_samples, )).view((-1, 1))\n",
    "    z = model(x)\n",
    "\n",
    "#\n",
    "a_z, b_z = beta_params(z)\n",
    "#\n",
    "dist_aprx = Beta(a_z, b_z)\n",
    "plot_beta_pdf(dist_aprx, \"aprx\")\n",
    "#\n",
    "print(\"pred\", a_z, b_z)\n",
    "print(\"true\",a_true, b_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c8080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
