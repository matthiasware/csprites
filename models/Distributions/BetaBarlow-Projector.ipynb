{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee50489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from plot_utils import *\n",
    "from BTwins.utils import calc_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d962cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Beta\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from dotted_dict import DottedDict\n",
    "import pprint\n",
    "#\n",
    "from utils import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695496e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMDist():\n",
    "    def __init__(self, mus, stds, alphas):\n",
    "        assert mus.shape == stds.shape\n",
    "        assert alphas.sum() == 1\n",
    "        if len(mus.shape) == 1:\n",
    "            mus = mus.reshape((1, -1))\n",
    "            stds = sts.reshape((1, -1))\n",
    "        assert mus.shape[0] == len(alphas)\n",
    "        self.mus = mus\n",
    "        self.stds = stds\n",
    "        self.alphas = alphas\n",
    "        #\n",
    "        self.dist_cat = torch.distributions.Categorical(alphas)\n",
    "    \n",
    "    def sample(self, n, shuffle=True):\n",
    "        y = self.dist_cat.sample((n,))\n",
    "        n_subsamples = torch.bincount(y)\n",
    "        y = y.sort()[0]\n",
    "        x = []\n",
    "        for idx, n_sub in enumerate(n_subsamples):\n",
    "            x.append(torch.distributions.Normal(self.mus[idx], self.stds[idx]).sample((n_sub,)))\n",
    "        x = torch.cat(x)\n",
    "        if shuffle:\n",
    "            idcs = torch.randperm(x.size()[0]) \n",
    "            x = x[idcs]\n",
    "            y = y[idcs]\n",
    "        return x, y\n",
    "    \n",
    "    def sample_barlow(self, n, shuffle=True):\n",
    "        y = self.dist_cat.sample((n,))\n",
    "        n_subsamples = torch.bincount(y)\n",
    "        y = y.sort()[0]\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        for idx, n_sub in enumerate(n_subsamples):\n",
    "            x1.append(torch.distributions.Normal(self.mus[idx], self.stds[idx]).sample((n_sub,)))\n",
    "            x2.append(torch.distributions.Normal(self.mus[idx], self.stds[idx]).sample((n_sub,)))\n",
    "        x1 = torch.cat(x1)\n",
    "        x2 = torch.cat(x2)\n",
    "        if shuffle:\n",
    "            idcs = torch.randperm(x1.size()[0]) \n",
    "            x1 = x1[idcs]\n",
    "            x2 = x2[idcs]\n",
    "            y = y[idcs]\n",
    "        return (x1, x2), y\n",
    "\n",
    "def get_normalizer(x):\n",
    "    norm_mean = x.mean(axis=0)\n",
    "    norm_std = (x - norm_mean).std(axis=0)\n",
    "    #\n",
    "    def normalize(samples):\n",
    "        return (samples - norm_mean) / norm_std\n",
    "    return normalize\n",
    "\n",
    "def normalize_z(z, eps):\n",
    "    return (z - z.mean(axis=0)) / (torch.sqrt(z.var(axis=0)) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [2, 4, 8, 16, 32, 64, 128, 512]:\n",
    "    lmbda = calc_lambda(d)\n",
    "    g = d / ((d**2 - d) * lmbda)\n",
    "    h = ((d**2 - d) * lmbda) / d\n",
    "    print(\"{:>4d}: {:8.4f} on/off={:.4f} off/on={:8.4f}\".format(d,lmbda, g, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c55f07",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = \"turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d17827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = torch.Tensor([\n",
    "    [2, 2],\n",
    "    [-2,1],\n",
    "    [0, -1]\n",
    "])\n",
    "stds = torch.Tensor([\n",
    "    [0.1, 0.1],\n",
    "    [0.1, 0.4],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "alphas = torch.Tensor([1/3, 1/3, 1/3])\n",
    "dist_in = GMDist(mus, stds, alphas)\n",
    "#\n",
    "n_samples = 1000\n",
    "x,y = dist_in.sample(n_samples)\n",
    "#\n",
    "normalizer = get_normalizer(x)\n",
    "x_normalized = normalizer(x)\n",
    "#\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=cmap)\n",
    "plt.scatter(x_normalized[:,0], x_normalized[:, 1],c=y, cmap=cmap)\n",
    "#\n",
    "print(x.mean(axis=0), x.var(axis=0))\n",
    "print(x_normalized.mean(axis=0), x_normalized.var(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1, x2), y = dist_in.sample_barlow(n_samples)\n",
    "plt.scatter(x1[:,0], x1[:,1], c=y)\n",
    "plt.show()\n",
    "#\n",
    "plt.scatter(x2[:,0], x2[:,1], c=y, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4509",
   "metadata": {},
   "source": [
    "# Beta-Barlows with Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f11f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation):\n",
    "    if activation == \"ReLU\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif activation == \"Sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    else:\n",
    "        raise NotImplementedError(activation)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, d_in, d_out, batch_norm=True, activation=None, bias=False):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(d_in, d_out, bias=bias)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(d_out))\n",
    "        if activation is not None:\n",
    "            layers.append(get_activation(activation))\n",
    "        \n",
    "        self.ff = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, n_hid, d_out,\n",
    "                 batch_norm=False, activation_last=\"ReLU\", batch_norm_last=True, bias=True):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.n_hid = n_hid\n",
    "        self.d_hid = d_hid\n",
    "        \n",
    "        self.dims = [d_in] + [d_hid] * n_hid\n",
    "        layers = []\n",
    "        for idx in range(len(self.dims) - 1):\n",
    "            layers.append(BasicBlock(self.dims[idx], self.dims[idx + 1], batch_norm=batch_norm, activation=\"ReLU\", bias=bias))\n",
    "        layers.append(BasicBlock(self.dims[-1], d_out, batch_norm_last, activation_last, bias=bias))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.dim_out = d_out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class BetaBarlow(nn.Module):\n",
    "    def __init__(self, backbone, beta_proj, barlow_proj):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.beta_proj = beta_proj\n",
    "        self.barlow_proj = barlow_proj\n",
    "        #\n",
    "        self.bn = nn.BatchNorm1d(barlow_proj.dim_out, affine=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.beta_proj(self.backbone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da286f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT DATA\n",
    "mus = torch.Tensor([\n",
    "    [2, 2],\n",
    "    [-2,1],\n",
    "    [0, -1],\n",
    "    [3, 5],\n",
    "])\n",
    "stds = torch.Tensor([\n",
    "    [0.4, 0.2],\n",
    "    [0.2, 0.7],\n",
    "    [0.3, 0.3],\n",
    "    [0.1, 0.2],\n",
    "])\n",
    "alphas = torch.Tensor([0.25, 0.25, 0.25, 0.25])\n",
    "dist_in = GMDist(mus, stds, alphas)\n",
    "#\n",
    "x,y = dist_in.sample(10000)\n",
    "normalizer = get_normalizer(x)\n",
    "\n",
    "x,y = dist_in.sample(10000)\n",
    "xt = normalizer(x)\n",
    "#\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].scatter(xt[:,0], xt[:,1], c=y, cmap=\"turbo\")\n",
    "axes[0].set_title(\"Unnormalized\")\n",
    "axes[1].scatter(x[:,0], x[:,1], c=y, cmap=\"turbo\")\n",
    "axes[1].set_title(\"Normalized\")\n",
    "plt.show()\n",
    "#\n",
    "print(x.mean(axis=0), x.var(axis=0))\n",
    "print(xt.mean(axis=0), xt.var(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global config\n",
    "DIM_IN  = mus.shape[1]\n",
    "DIM_BB  = 64\n",
    "DIM_OUT = 8\n",
    "N_BACKBONE = 5\n",
    "N_BETAPROJ = 1\n",
    "N_BARLOWPROJ = 3\n",
    "#\n",
    "config = {\n",
    "    \"train\":\n",
    "    {\n",
    "        \"batch_size\": 512,\n",
    "        \"num_steps\": 500,\n",
    "        \"num_epochs\": 20,\n",
    "        \"plot_freq\": 5,\n",
    "        \"lr\": 0.001,\n",
    "    },\n",
    "    \"loss\":\n",
    "    {\n",
    "        \"barlow\": {\n",
    "            \"w_off\": 41,                     # used with mean\n",
    "            \"lmbda\": calc_lambda(DIM_OUT),   # used with sum\n",
    "            \"eps\": 1e-8,\n",
    "        },\n",
    "        \"beta\":\n",
    "        {\n",
    "            \"a_true\": 0.1,\n",
    "            \"b_true\": 0.5,\n",
    "        },\n",
    "        \"w_barlow\": 2,\n",
    "        \"w_beta\": 1\n",
    "    },\n",
    "    \"backbone\":\n",
    "    {\n",
    "        \"d_in\": DIM_IN,\n",
    "        \"d_out\": DIM_BB,\n",
    "        \"d_hid\": DIM_BB,\n",
    "        \"n_hid\": N_BACKBONE - 1,\n",
    "        \"batch_norm\": True,\n",
    "        \"bias\": True,\n",
    "        \"activation_last\": \"ReLU\",\n",
    "        \"batch_norm_last\": True,\n",
    "    },\n",
    "    \"beta_proj\":{\n",
    "        \"bias\": False,\n",
    "        \"d_in\": DIM_BB,\n",
    "        \"n_hid\": N_BETAPROJ - 1,\n",
    "        \"d_hid\": DIM_BB,\n",
    "        \"d_out\": DIM_OUT,\n",
    "        \"batch_norm\": True,\n",
    "        \"activation_last\": \"Sigmoid\",\n",
    "        \"batch_norm_last\": False,\n",
    "    },\n",
    "    \"barlow_proj\":\n",
    "    {\n",
    "        \"bias\": False,\n",
    "        \"d_in\": DIM_OUT,\n",
    "        \"d_out\": DIM_OUT,\n",
    "        \"n_hid\": N_BARLOWPROJ - 1,\n",
    "        \"d_hid\": DIM_OUT,\n",
    "        \"batch_norm\": True,\n",
    "        \"batch_norm_last\": False,\n",
    "        \"activation_last\": None\n",
    "    }\n",
    "}\n",
    "config = DottedDict(config)\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BETA LOSS\n",
    "a_true, b_true = torch.Tensor([config.loss.beta.a_true, config.loss.beta.b_true])\n",
    "dist_true = Beta(a_true, b_true)\n",
    "plot_beta_pdf(dist_true, \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BetaBarlow(\n",
    "    backbone = Net(**config.backbone),\n",
    "    beta_proj = Net(**config.beta_proj),\n",
    "    barlow_proj = Net(**config.barlow_proj)\n",
    "    \n",
    ")\n",
    "print(model.barlow_proj)\n",
    "print(model.beta_proj)\n",
    "print(model.backbone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = dist_in.sample(1000)\n",
    "x_valid, y_valid = dist_in.sample(1000)\n",
    "#\n",
    "x_train = normalizer(x_train)\n",
    "x_valid = normalizer(x_valid)\n",
    "#\n",
    "clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "print(\"LR(X)\",clf.score(x_valid, y_valid))\n",
    "#\n",
    "model = BetaBarlow(\n",
    "    backbone = Net(**config.backbone),\n",
    "    beta_proj = Net(**config.beta_proj),\n",
    "    barlow_proj = Net(**config.barlow_proj)\n",
    "    \n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z_train = model(x_train)\n",
    "    z_valid = model(x_valid)\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(z_train, y_train)\n",
    "print(\"LR(Z)\",clf.score(z_valid, y_valid))\n",
    "#\n",
    "a_z, b_z = beta_params(z_train)\n",
    "for idx in range(z_train.shape[1]):\n",
    "    title = \"Z_{}, alpha={:.3f}, beta={:.3f}\".format(idx, a_z[idx].item(), b_z[idx].item())\n",
    "    simplex_plot(z_train[:,idx].detach().numpy(), title=title, c=y_train, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f97f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = BetaBarlow(\n",
    "    backbone = Net(**config.backbone),\n",
    "    beta_proj = Net(**config.beta_proj),\n",
    "    barlow_proj = Net(**config.barlow_proj)\n",
    "    \n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.train.lr)\n",
    "#\n",
    "all_loss = []\n",
    "all_a = []\n",
    "all_b = []\n",
    "for epoch_idx in range(1, config.train.num_epochs + 1, 1):\n",
    "    # ##########\n",
    "    # TRAIN\n",
    "    # ##########\n",
    "    model.train()\n",
    "    desc = \"[{:3}/{:3}]\".format(epoch_idx, config.train.num_epochs)\n",
    "    pbar = tqdm(range(config.train.num_steps), bar_format= desc + '{bar:10}{n_fmt}/{total_fmt}{postfix}')\n",
    "    epoch_loss = 0\n",
    "    for step in pbar:\n",
    "        (x1, x2), _ = dist_in.sample_barlow(config.train.batch_size)\n",
    "        x1 = normalizer(x1)\n",
    "        x2 = normalizer(x2)\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        z1 = model.beta_proj(model.backbone(x1))\n",
    "        z2 = model.beta_proj(model.backbone(x2))\n",
    "        \n",
    "        # BETA LOSS\n",
    "        a_z, b_z = beta_params(torch.cat([z1, z2], axis=0))\n",
    "        loss_beta = kl_beta_beta((a_z,b_z),(a_true,b_true),forward=True).sum()\n",
    "        \n",
    "        # BARLOW LOSS\n",
    "        z1 = model.barlow_proj(z1)\n",
    "        z2 = model.barlow_proj(z2)\n",
    "        #\n",
    "        z1_norm = model.bn(z1)\n",
    "        z2_norm = model.bn(z2)\n",
    "        #\n",
    "        #z1_norm = beta_normalize(z1, dist_true.mean, dist_true.stddev)\n",
    "        #z2_norm = beta_normalize(z2, dist_true.mean, dist_true.stddev)\n",
    "        #\n",
    "        c = z1_norm.T @ z2_norm\n",
    "        c.div_(z1.shape[0])\n",
    "        \n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "        off_diag = off_diagonal(c).pow_(2).sum()\n",
    "        loss_barlow = on_diag + config.loss.barlow.lmbda * off_diag\n",
    "        \n",
    "        # LOSS\n",
    "        loss = config.loss.w_barlow * loss_barlow + config.loss.w_beta * loss_beta\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix(\n",
    "              {'L': loss.item(),\n",
    "               'on': on_diag.item(),\n",
    "               'off': off_diag.item(),\n",
    "               'dkl': loss_beta.item(),\n",
    "               'a_min': a_z.min().item(),\n",
    "               'a_max': a_z.max().item(),\n",
    "               'b_min': b_z.min().item(),\n",
    "               'b_max': b_z.max().item()\n",
    "               }\n",
    "          )\n",
    "\n",
    "    all_loss.append(epoch_loss / config.train.num_steps)\n",
    "    ############\n",
    "    # EVAL\n",
    "    ############\n",
    "    if epoch_idx % config.train.plot_freq == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, y = dist_in.sample(config.train.batch_size)\n",
    "            x = normalizer(x)\n",
    "            z = model(x)\n",
    "            a_z, b_z = beta_params(z)\n",
    "            #\n",
    "            for idx in range(z.shape[1]):\n",
    "                title = \"Z_{}, alpha={:.3f}, beta={:.3f}\".format(idx, a_z[idx].item(), b_z[idx].item())\n",
    "                simplex_plot(z[:,idx].detach().numpy(), title=title, c=y, cmap=cmap)\n",
    "            \n",
    "            x_train,y_train = dist_in.sample(1000)\n",
    "            x_valid, y_valid = dist_in.sample(100)\n",
    "            #\n",
    "            x_train = normalizer(x_train)\n",
    "            x_valid = normalizer(x_valid)\n",
    "            #\n",
    "            z_train = model(x_train)\n",
    "            z_valid = model(x_valid)\n",
    "            #\n",
    "            clf = LogisticRegression(random_state=0).fit(z_train, y_train)\n",
    "            print(\"   LINPROB: {:.3f}\".format(clf.score(z_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41efefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cm.get_cmap(cmap)(y_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(z.shape[1]), z.mean(axis=0), width=1.0)\n",
    "plt.show()\n",
    "plt.bar(range(z.shape[0]), z.mean(axis=1), width=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdb3b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for clz in range(mus.shape[0]):\n",
    "    print(\"*\"*100)\n",
    "    print(\"CLASS: {}\".format(clz))\n",
    "    print(\"*\"*100)\n",
    "    for idx in range(z.shape[1]):\n",
    "        show_idcs = (y == clz)\n",
    "        z_show = z[show_idcs]\n",
    "        y_show = y[show_idcs]\n",
    "        title = \"Z_{}, alpha={:.3f}, beta={:.3f}\".format(idx, a_z[idx].item(), b_z[idx].item())\n",
    "        simplex_plot(z_show[:,idx].detach().numpy(), title=title, c=plt.cm.get_cmap(cmap)(y_show))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137064f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277161f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
