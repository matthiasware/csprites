{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Beta\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d525920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_plot1d(x, figsize=(10, 1)):\n",
    "    x = np.array(x).squeeze()\n",
    "    assert len(x.shape) == 1\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x, np.ones(len(x)))\n",
    "    plt.plot([0, 1], [1, 1])\n",
    "    plt.xlim([-0.2, 1.2])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_beta_pdf(dist, title=None):\n",
    "    xx = torch.linspace(0, 1, 200)[1:-1]\n",
    "    plt.plot(xx, torch.exp(dist.log_prob(xx)))\n",
    "    a, b = float(dist.concentration0), float(dist.concentration1)\n",
    "    if title is not None:\n",
    "        plt.title(\"{} \\n a={:.3f}, beta={:.3f}\".format(\n",
    "            title, a, b))\n",
    "    else:\n",
    "        plt.title(\"a={:.3f}, beta={:.3f}\".format(a, b))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def beta_params(X):\n",
    "    mu = X.mean()\n",
    "    var = X.var()\n",
    "    #\n",
    "    a = ((mu * (1 - mu)) / var - 1) * mu\n",
    "    b = ((mu * (1 - mu)) / var - 1) * (1 - mu)\n",
    "    return a, b\n",
    "\n",
    "def beta_params2(X):\n",
    "    mu = X.mean()\n",
    "    var = X.var()\n",
    "    #\n",
    "    a = ((1 - mu) / var - (1 / mu)) * mu**2\n",
    "    b = a * (1 / mu - 1)\n",
    "    return a, b\n",
    "\n",
    "def kl_beta_beta_pt(p, q):\n",
    "    sum_params_p = p.concentration1 + p.concentration0\n",
    "    sum_params_q = q.concentration1 + q.concentration0\n",
    "    t1 = q.concentration1.lgamma() + q.concentration0.lgamma() + (sum_params_p).lgamma()\n",
    "    t2 = p.concentration1.lgamma() + p.concentration0.lgamma() + (sum_params_q).lgamma()\n",
    "    t3 = (p.concentration1 - q.concentration1) * torch.digamma(p.concentration1)\n",
    "    t4 = (p.concentration0 - q.concentration0) * torch.digamma(p.concentration0)\n",
    "    t5 = (sum_params_q - sum_params_p) * torch.digamma(sum_params_p)\n",
    "    return t1 - t2 + t3 + t4 + t5\n",
    "\n",
    "def kl_beta_beta(ab_aprx, ab_true, forward=True):\n",
    "    \"\"\"\n",
    "    Calculates either:\n",
    "        Forward KL: D_kl(P||Q)\n",
    "        Reverse KL: D_kl(Q||P)\n",
    "    where:\n",
    "        P ... True distribution\n",
    "        Q ... Approximation\n",
    "    Forward:\n",
    "        - Mean seeking\n",
    "        - Where pdf(P) is high, pdf(Q) must be high\n",
    "    Reverse:\n",
    "        - Mode seeking\n",
    "        - where pdf(Q) is high, pdf(P) must be high\n",
    "    \"\"\"\n",
    "    if forward:\n",
    "        p_a, p_b = ab_aprx\n",
    "        q_a, q_b = ab_true\n",
    "    else:\n",
    "        p_a, p_b = ab_true\n",
    "        q_a, q_b = ab_aprx\n",
    "    #\n",
    "    sum_pab = p_a + p_b\n",
    "    sum_qab = q_a + q_b\n",
    "    #\n",
    "    t1 = q_b.lgamma() + q_a.lgamma() + (sum_pab).lgamma()\n",
    "    t2 = p_b.lgamma() + p_a.lgamma() + (sum_qab).lgamma()\n",
    "    t3 = (p_b - q_b) * torch.digamma(p_b)\n",
    "    t4 = (p_a - q_a) * torch.digamma(p_a)\n",
    "    t5 = (sum_qab - sum_pab) * torch.digamma(sum_pab)\n",
    "    return t1 - t2 + t3 + t4 + t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, b1 = torch.Tensor([8]), torch.Tensor([2])\n",
    "a2, b2 = 1/a1, 1/b1\n",
    "#\n",
    "d1 = Beta(a1, b1)\n",
    "d2 = Beta(a2, b2)\n",
    "#\n",
    "n_samples = 100\n",
    "x1 = d1.sample((n_samples, ))\n",
    "x2 = d2.sample((n_samples, ))\n",
    "#\n",
    "beta_plot1d(x1)\n",
    "beta_plot1d(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_beta_beta_pt(d2, d1))\n",
    "print(kl_beta_beta_pt(d1, d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kl_beta_beta(ab_aprx=(a2, b2), ab_true=(a1, b1), forward=True))\n",
    "print(kl_beta_beta(ab_aprx=(a2, b2), ab_true=(a1, b1), forward=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "#\n",
    "a_true, b_true = 0.2, 0.5\n",
    "d_true = Beta(a_true, b_true)\n",
    "#\n",
    "d_aprx = Beta(1/b_true, 1/a_true)\n",
    "x_aprx = d_aprx.sample((n_samples,))\n",
    "#\n",
    "a_aprx, b_aprx = beta_params(x_aprx)\n",
    "a_aprx, b_aprx = 1/b_aprx, 1/a_aprx\n",
    "#\n",
    "print(\"P = Beta({:.3f},{:.3f})\".format(a_true, b_true))\n",
    "print(\"Q = Beta({:.3f},{:.3f})\".format(a_aprx, b_aprx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta_pdf(d_true, title=\"True\")\n",
    "plot_beta_pdf(d_aprx, title=\"Approx\")\n",
    "plot_beta_pdf(d_aprx, title=\"Estimated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df181cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, b1 = torch.Tensor([0.2]), torch.Tensor([0.8])\n",
    "#a2, b2 = 1/a1, 1/b1\n",
    "a3, b3 = 1/b1, 1/a1\n",
    "\n",
    "d1 = Beta(a1, b1)\n",
    "#d2 = Beta(a2, b2)\n",
    "d3 = Beta(a3, b3)\n",
    "#\n",
    "plot_beta_pdf(d1)\n",
    "#plot_beta_pdf(d2, title=\"GW\")\n",
    "plot_beta_pdf(d3, title=\"GW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4292b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, d_in, d_out, batch_norm):\n",
    "        super().__init__()\n",
    "        if batch_norm:\n",
    "            self.ff = nn.Sequential(\n",
    "                nn.Linear(d_in, d_out),\n",
    "                nn.BatchNorm1d(d_out),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.ff = nn.Sequential(\n",
    "                nn.Linear(d_in, d_out),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, n_hid, d_out, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.n_hid = n_hid\n",
    "        self.d_hid = d_hid\n",
    "        \n",
    "        self.dims = [d_in] + [d_hid] * n_hid\n",
    "        \n",
    "        layers = []\n",
    "        for idx in range(len(self.dims) - 1):\n",
    "            layers.append(BasicBlock(self.dims[idx], self.dims[idx + 1], batch_norm))\n",
    "        layers.append(nn.Linear(self.dims[-1], d_out))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "def beta_params(X):\n",
    "    mu = X.mean()\n",
    "    var = X.var()\n",
    "    #\n",
    "    a = ((mu * (1 - mu)) / var - 1) * mu\n",
    "    b = ((mu * (1 - mu)) / var - 1) * (1 - mu)\n",
    "    return a, b\n",
    "\n",
    "def kl_beta_beta(ab_aprx, ab_true, forward=True):\n",
    "    \"\"\"\n",
    "    Calculates either:\n",
    "        Forward KL: D_kl(P||Q)\n",
    "        Reverse KL: D_kl(Q||P)\n",
    "    where:\n",
    "        P ... True distribution\n",
    "        Q ... Approximation\n",
    "    Forward:\n",
    "        - Mean seeking\n",
    "        - Where pdf(P) is high, pdf(Q) must be high\n",
    "    Reverse:\n",
    "        - Mode seeking\n",
    "        - where pdf(Q) is high, pdf(P) must be high\n",
    "    \"\"\"\n",
    "    if forward:\n",
    "        p_a, p_b = ab_aprx\n",
    "        q_a, q_b = ab_true\n",
    "    else:\n",
    "        p_a, p_b = ab_true\n",
    "        q_a, q_b = ab_aprx\n",
    "    #\n",
    "    sum_pab = p_a + p_b\n",
    "    sum_qab = q_a + q_b\n",
    "    #\n",
    "    t1 = q_b.lgamma() + q_a.lgamma() + (sum_pab).lgamma()\n",
    "    t2 = p_b.lgamma() + p_a.lgamma() + (sum_qab).lgamma()\n",
    "    t3 = (p_b - q_b) * torch.digamma(p_b)\n",
    "    t4 = (p_a - q_a) * torch.digamma(p_a)\n",
    "    t5 = (sum_qab - sum_pab) * torch.digamma(sum_pab)\n",
    "    return t1 - t2 + t3 + t4 + t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 1\n",
    "d_hid = 16\n",
    "d_out = 1\n",
    "n_hid = 4\n",
    "batch_norm = True\n",
    "#\n",
    "a_true, b_true = torch.Tensor([0.8, 0.8])\n",
    "#\n",
    "a_true_prox = 1 / b_true\n",
    "b_true_prox = 1 / a_true\n",
    "#\n",
    "dist_true = Beta(a_true, b_true)\n",
    "dist_true_prox = Beta(a_true_prox, b_true_prox)\n",
    "#\n",
    "plot_beta_pdf(dist_true, \"True\")\n",
    "plot_beta_pdf(dist_true_prox, \"Prox\")\n",
    "#\n",
    "dist_in = torch.distributions.Uniform(0, 1)\n",
    "x = dist_in.sample((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4be247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "model.apply(init_weights)\n",
    "x = torch.rand((512, d_in))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x)\n",
    "#\n",
    "beta_plot1d(z.detach().numpy())\n",
    "beta_plot1d(x.numpy())\n",
    "print(z.min(), z.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(d_in=d_in, d_hid=d_hid, n_hid=n_hid, d_out=d_out, batch_norm=batch_norm)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e78bc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_steps = 200\n",
    "num_epochs = 10\n",
    "for epoch_idx in range(1, num_epochs + 1, 1):\n",
    "    model.train()\n",
    "    desc = \"Epoch [{:3}/{:3}] {}:\".format(epoch_idx, num_epochs, 'train')\n",
    "    pbar = tqdm(range(num_steps), bar_format= desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    for step in pbar:\n",
    "        x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "        # ##########\n",
    "        # TRAIN\n",
    "        # ##########\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        z = model(x)\n",
    "        a_z, b_z = beta_params(z)\n",
    "        a_aprx_prox = 1 / b_z\n",
    "        b_aprx_prox = 1 / a_z\n",
    "        loss =  kl_beta_beta((a_aprx_prox,b_aprx_prox),\n",
    "                                 (a_true_prox,b_true_prox),forward=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        pbar.set_postfix({'loss': loss.item(), 'a': a_z.item(), 'b': b_z.item()})\n",
    "        time.sleep(0.01)\n",
    "    ############\n",
    "    # EVAL\n",
    "    ############\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = dist_in.sample((batch_size, )).view((-1, 1))\n",
    "        z = model(x)\n",
    "        #\n",
    "        a_z, b_z = beta_params(z)\n",
    "        dist_aprx = Beta(a_z, b_z)\n",
    "        plot_beta_pdf(dist_aprx, \"aprx\")\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35beb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "n_samples = 64\n",
    "with torch.no_grad():\n",
    "    x = dist_in.sample((n_samples, )).view((-1, 1))\n",
    "    z = model(x)\n",
    "beta_plot1d(z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000\n",
    "with torch.no_grad():\n",
    "    x = dist_in.sample((n_samples, )).view((-1, 1))\n",
    "    z = model(x)\n",
    "\n",
    "#\n",
    "a_z, b_z = beta_params(z)\n",
    "#\n",
    "dist_aprx = Beta(a_z, b_z)\n",
    "plot_beta_pdf(dist_aprx, \"aprx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_z, b_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c63830",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true, b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42218dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
